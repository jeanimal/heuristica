---
title: "Confusion Matrix"
author: "Jean Czerlinski Whitmore"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Confusion Matrix}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r}
library(heuristica)
```


This is a topic for advanced users.

Predicting between pairs produces categorical output: -1, 0, or 1.  A (confusion matrix)[https://en.wikipedia.org/wiki/Confusion_matrix] shows how often the predictor matched each category.  For example, it can be enlightening to see that one heuristic had to guess (predict 0) more often than another.  The confusion matrix also allows us to calculate various stats, such as the accuracy (which heuristica already reports), sensitivity (true positive / all positive), specificity (true negative / all negative) and precision (true positive / predicted positive).  However, there are some quirks to applying confusion matrices to this task, and this vignette explains them.

* Most heuristica functions exploit the fact that row pairs are inherently symmetric (A vs B = - B vs. A), so we need to run reverse row pairs to generate the correct data for a confusion matrix.
* Guesses and ties need to be categorized as 1 or -1 for measures like accuracy.

# Running reverse row pairs

```{r}
data("city_population")
data_set <- na.omit(city_population)
criterion_col <- 3
cols_to_fit <- 4:ncol(data_set)
```

Fit Take the Best and regression on a subset of the data.  (For this example, some interested rows were selected, but in practice the training rows would be randomly sampled.  Also, in practice we would want to check predictions on the non-training data.)

```{r}
num_training_rows <- 5
train_data <- city_population[c(3:(3+num_training_rows)),]
ttb <- ttbModel(train_data, criterion_col, cols_to_fit)
reg <- regModel(train_data, criterion_col, cols_to_fit)
lreg <- logRegModel(train_data, criterion_col, cols_to_fit)
```

## Analyzing just "forward" row pairs

We normally use `predictPairSummary`, but that efficiently applies the functions to only one set of unique row pairs.  For example, the output below shows CorrectGreater as only having the output of 1 because the data is sorted in descening order.  But for a confusion matrix, we also want to count what the heuristics would have predicted for -1 rows.

```{r}
out_fwd_row_pairs_only <- predictPairSummary(train_data, ttb, reg, lreg)
out_fwd_row_pairs_only
```

Just for kicks-- even though it's incomplete-- let's look at a confusion matrix for TakeTheBest.  Notice only the last row, named "1" has values.  That is because the only category in "ref_data" was "1".  (The function still produces the other rows so that we always get the same 3x3 shape for analysis.)

```{r}
ref_data <- out_fwd_row_pairs_only[,"CorrectGreater"]
predictions <- out_fwd_row_pairs_only[,"ttbModel"]
confusionMatrixFor_Neg1_0_1(ref_data, predictions)
```

The fix is to generate row pairs for "backward" row pairs, too, e.g. Row2 vs. Row 1 in addition to Row 1 vs. Row 2.  We will now see three ways to do that, beginning with more explicit and manual ways.

## Generating "reverse" row pairs

There are several ways to generate the confusion matrix to include the "reverse" row pairs. 

The most straightforward way to handle this is to run predictPairSummary twice, once with the rows in the usual "forward" order and then again with the rows in "reverse order."  Below we see that CorrectGreater now includes -1's.

```{r}
out_fwd_row_pairs_only <- predictPairSummary(train_data, ttb, reg, lreg)
train_data_rev_rows <- train_data[c(nrow(train_data):1),]
out_rev_row_pairs_only <- predictPairSummary(train_data_rev_rows, ttb, reg, lreg)
out <- rbind(out_fwd_row_pairs_only, out_rev_row_pairs_only)
out
```

With that output, we can generate a complete confusion matrix for Take The Best with non-zero data in both the first "-1" row and the last "1" row.

```{r}
ref_data <- out[,"CorrectGreater"]
predictions <- out[,"ttbModel"]
confusionMatrixFor_Neg1_0_1(ref_data, predictions)
```

You might also notice a symmetry.  For example, there are 7 cases where TTB accurately predicted 1 where the value was 1.  When these rows were reversed, they resulted in 7 cases where TTB accurately predicted -1 where the correct value was -1.  This happens because TTB is a symmetric model: predictPair(Row1, Row2) = - predictPair(Row2, Row1).  All the models included in heuristica have this property.  We can exploit it directly in calculating the confusion matrix.  A hand-coded example of copying the values from one row is shown below first.  But note that the results for the reversed rows can be obtained more generally by reversing all rows and then also reversing all columns.  If we sum the original matrix and the reversed row reversed column matrix, we obtain a matrix with all the counts.

```{r}
ref_data <- out_fwd_row_pairs_only[,"CorrectGreater"]
predictions <- out_fwd_row_pairs_only[,"ttbModel"]
ttb_fwd_confusion_matrix <- confusionMatrixFor_Neg1_0_1(ref_data, predictions)
ttb_confusion_matrix <- ttb_fwd_confusion_matrix
# The lines below copy over the few values we need for this matrix.
ttb_confusion_matrix["-1", "-1"] <- ttb_fwd_confusion_matrix["1", "1"]
ttb_confusion_matrix["-1", "0"] <- ttb_fwd_confusion_matrix["1", "0"]
ttb_confusion_matrix["-1", "1"] <- ttb_fwd_confusion_matrix["1", "-1"]
ttb_confusion_matrix
# But here is a more generlizeable way.
ttb_fwd_confusion_matrix + reverseRowsAndReverseColumns(ttb_fwd_confusion_matrix)
```

Yet another way is to use the more flexible rowPairApplyList function rather than predictPairSummary to generate the prediction data because rowPairApplyList can take optional arguments to revere rows.  Specifically, add the optional setting `also_reverse_row_pairs=TRUE`.  

```{r}
out <- rowPairApplyList(train_data, list(correctGreater(criterion_col), heuristics(ttb, reg, lreg)), also_reverse_row_pairs=TRUE)
out
```

Calculating Take The Best's confusion matrix from this output produces the same result as above.

```{r}
ref_data <- out[,"CorrectGreater"]
predictions <- out[,"ttbModel"]
confusionMatrixFor_Neg1_0_1(ref_data, predictions)
```

## Analyzing output of the three models

Let's generate a confusion matrices for each heuristic and anlyze the output.
```{r}
confusion_matrix_3x3_ttb <- confusionMatrixFor_Neg1_0_1(out[,"CorrectGreater"], out[,"ttbModel"])
confusion_matrix_3x3_ttb

confusion_matrix_3x3_reg <- confusionMatrixFor_Neg1_0_1(out[,"CorrectGreater"], out[,"regModel"])
confusion_matrix_3x3_reg

confusion_matrix_3x3_lreg <- confusionMatrixFor_Neg1_0_1(out[,"CorrectGreater"], out[,"logRegModel"])
confusion_matrix_3x3_lreg
```

Take The Best does worse on this data because it guesses much more than the regression models-- 12 guesses (6+6) vs. only 4 (2+2).  When TTB is not guessing, it is highly accurate, getting 7 correct for every 2 incorrect, an excellent ratio of 3.5.  The regression models have a non-guessing correctness ration of 9 vs. 4 =  2.25.  We will see in the next section the impact thes numbers have on the percent correct, but we will need a way to deal with guesses to do that. 

It interesting that regression and logistic regression have the exact same confusion matrix, even though the output showed they sometimes disagreed.  Below are the cases where they diagreed, and we see that their correct and incorrect values exactly balance out.  Notice also that these were rows that Take The Best guessed on, deeming them too hard to distinguish.
```{r}
out_df <- data.frame(out)
out_df[out_df$regModel != out_df$logRegModel,]
```

# Distributing guesses and ties to get stats

In order to calculate percentCorrect from the confusion matrix, we need to handle the guesses.  Heuristica offers a function that allocates thes by their expected values, so half the guess counts are moved to +1 and half are moved to -1.  Then the guess row can be removed.  Below we see the original matrix and how it looks after having guesses distributed by `collapseConfusionMatrix3x3To2x2`.  (Likewise `collapseConfusionMatrix3x3To2x2` distributes half the ties to +1 and half to -1, although in this data set there were no ties.)

```{r}
confusion_matrix_3x3_ttb
confusion_matrix_ttb <- collapseConfusionMatrix3x3To2x2(confusion_matrix_3x3_ttb)
confusion_matrix_ttb
```

The number of correct predictions is along the diagonal-- where correct was -1 and the prediction was -1 and where correct was 1 and the prediction was 1.  So the percent correct is the sum of the diagonal divided by the sum of the whole matrix.

```{r}
percent_correct_ttb <- 100 *sum(diag(confusion_matrix_ttb)) / sum(confusion_matrix_ttb)
percent_correct_ttb
```

This agrees with the output of heuristica's one-step `percentCorrect` function would tell us.
```{r}
percentCorrect(train_data, ttb)
```

Now distribute guesses for the regression models.  Funnily enough, we end up with the same confusion matrix and accuracy as Take The Best.  In other words, the additional "predictions" that regression models made did no better than Take The Best's guesses!  (Perhaps a regression user would be "overconfident.")

```{r}
confusion_matrix_3x3_reg
confusion_matrix_reg <- collapseConfusionMatrix3x3To2x2(confusion_matrix_3x3_reg)
confusion_matrix_reg
```

## Statistics

So why bother with the confusion matrix?  Because it gives us insight into the details of how the algorithms achieve their percent correct.  We now know that Take The Best guessed 3 times more on this data set than regressions models.  And with the confusion matrix, we can calculate a variety of stats in addition to accuracy:

* accuracy = (true positive + true negatve) / all (100 times this is the same as percentCorrect)
* sensitivity = true pasitive rate = true positive / all positive (sensitivity is also called recall)
* specificity = true negative rate = true negative / all negative
* precision = positive predictive velue = true positive rate

Since all the models ended up with the same confusion matrix, we only need to calculate these stats once.  But when models differ, the stats can be enlightening.

```{r}
statsFromConfusionMatrix(confusion_matrix_ttb)
```

Note that due to the row reversing, sensitivity and specificity will always be the same for symmetric heuristics.  The others are just accidentally the same on this data set.

