---
title: "Confusion Matrix"
author: "Jean Czerlinski Whitmore"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Confusion Matrix}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r}
library(heuristica)
```


This is a topic of advanced users.

Predicting between pairs produces categorical output: -1, 0, or 1.  Using a (confusion matrix)[https://en.wikipedia.org/wiki/Confusion_matrix] allows us to see how often the predicted category aligns with the correct category.  In the case of heuristica simulations, it can be useful to see if one heuristic has to guess (predict 0) more often than another.  The confusion matrix also allows us to calculate various stats, such as the accuracy (which heuristica already reports), sensitivity (true positive / all positive), specificity (true negative / all negative) and precision (true positive / predicted positive).  However, there are some quirks to applying confusion matrices to this task, and this vignette explains them.


#' accuracy = (true positive + true negatve) / all
#' sensitivity = true pasitive rate = true positive / all positive
#'   (sensitivity is also called recall)
#' specificity = true negative rate = true negative / all negative
#' precision = positive predictive velue = true positive

# City population example

```{r}
data("city_population")
data_set <- na.omit(city_population)
criterion_col <- 3
cols_to_fit <- 4:ncol(data_set)
```

Fit Take the Best and regression on a subset of the data.  For this example, we arbitrarily pick the first N rows rather than sampling, but in practice the training rows should be sampled.

```{r}
num_training_rows <- 5
train_data <- city_population[c(1:num_training_rows),]
ttb <- ttbModel(train_data, criterion_col, cols_to_fit)
reg <- regModel(train_data, criterion_col, cols_to_fit)
```


We want something like the output of `predictPairSummary`, but as I'll soon explain, we need to run reverse rows, too.

So generate predictions using rowPairApplyList, which can take optional arguments.

We pass it the full data set.  Also include correctGreater so we can assess how well the heuristics match the correct output.
Setting `also_reverse_row_pairs=TRUE` is key for analyzing the output.

```{r}
out <- rowPairApplyList(data_set, list(correctGreater(criterion_col), heuristics(ttb, reg)), also_reverse_row_pairs=TRUE)
```

Take a look at a few rows.
```{r}
head(out)
```

Generate confusion matrices for each heuristic, first Take The Best, then regression.
```{r}
confusion_matrix_3x3_ttb <- confusionMatrixPredictPair(out[,"CorrectGreater"], out[,"ttbModel"])
confusion_matrix_3x3_ttb

confusion_matrix_3x3_reg <- confusionMatrixPredictPair(out[,"CorrectGreater"], out[,"regModel"])
confusion_matrix_3x3_reg
```

First, notice that the first and third rows are the same.  This is becuase ttb and regression are symmetric heuristics.  When a symmetric heuristic chooses Berlin when given Berlin vs. Munich-- an output of "1" in predictPair-- it will still choose Berlin when given the reverse, Munich vs. Berlin-- and output of "-1" in predictPair.  It's also confirmation that we set `also_reverse_row_pairs=TRUE`.  If you forget to set this, the first and third rows might not be equal.

Second, notice that Take The Best guesses a lot more.  Focusing on the first row, Take The Best guessed 1,213 times while regression guessed roughly half as much 678 times.

Now view both matrices with guesses-- 0 predictions-- allocated.  There are different ways to allocate guesses, but by default this fucntion assignes 50% of them to correct and 50% to incorrect.
```{r}
confusion_matrix_ttb <- collapseConfusionMatrix3x3To2x2(confusion_matrix_3x3_ttb)
confusion_matrix_ttb
confusion_matrix_reg <- collapseConfusionMatrix3x3To2x2(confusion_matrix_3x3_reg)
confusion_matrix_reg
```


Calculate stats, including accuracy from these confusion matrices.

```{r}
statsFromConfusionMatrix(confusion_matrix_ttb)
statsFromConfusionMatrix(confusion_matrix_reg)
```


# Analyzing a Guttman environment

This is an even more advanced example inspired by the paper, "The Bias Bias" (citation below).

A confusion matrix compares the predicted values with the actual, correct values.  rowPairApply can generate these values, and here we walk through an example, comparing Take the Best and Greedy Take The Best on a Guttman environment, which has every possible binary cue of validity one.  This is one of the cases analyzed in the paper, "The Bias Bias" (citation below).

Here is a function to generate a Guttman environment:
```{r}
# Makes a Guttman binary data set.  E.g.
# > makeGuttman(3)
#      y   x1  x2 x3
# [1,] 4   1   1   1
# [2,] 3   0   1   1
# [3,] 2   0   0   1
# [4,] 1   0   0   0
# Not efficient, but it works.
makeGuttman <- function(num_cues) {
  num_rows <- num_cues + 1
  mat <- cbind(y=c(num_rows:1))
  for (num_1s in 1:num_cues) {
    num_0s <- num_rows - num_1s
    vec <- c(rep(1, num_1s), rep(0, num_0s))
    mat <- cbind(mat, vec)
    colnames(mat)[ncol(mat)] <- paste0("x", num_1s)
  }
  return(mat)
}
```

Now use it to generate a data set with 32 cues (33 rows).
```{r}
all_data <- makeGuttman(4)
```

Fit the Take The Best model and confirm it thinks all cue validities are 1.
```{r}
train_data <- all_data
criterion_col <- 1
ttb <- ttbModel(train_data, criterion_col, c(2:ncol(train_data)))
ttb$cue_validities
```

To generate data for the confusion matrix, we need correct values, which can be generated with correctGreater, and the predictions from the ttb model.  Generate it like this:

```{r}
out <- rowPairApplyList(train_data, list(correctGreater(criterion_col), heuristics(ttb)), also_reverse_row_pairs=FALSE)
```

Now send the data to the confusion matrix function.

```{r}
confusion_matrix <- confusionMatrixPredictPair(out[,"CorrectGreater"], out[,"ttbModel"])
#confusion_matrix
```



Now let's us

Notice the optional parameter `also_reverse_row_pairs=TRUE`.

Why is this wrong?  rowPairApply tries to be efficient, so be default it runs each unique row pair only once.  That is, if it applies a function to row1 vs. row2, it does not also apply it to row2 vs. row1.  That works well when both these conditions hold:

1) Your heuristics are symmetric, e.g. predictPair(, B, heuristic) = -predictPair(row1, row2, heuristic).  (A heuristic that always predicts 1 violatees this.)
2) Your goal is measuring accuracy, # correct predictions / # predictions.

Condition 2 is violated when calculating a confusion matrix because it shows specifically what happens for _all_ outcomes.  So in order to generate data for a confusion matrix, it is important to use the optional `also_reverse_row_pairs=TRUE` parameter.


