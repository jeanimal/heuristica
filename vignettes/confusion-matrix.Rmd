---
title: "Confusion Matrix"
author: "Jean Czerlinski Whitmore"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Confusion Matrix}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r}
library(heuristica)
```


This is a topic for advanced users.

Predicting between pairs produces categorical output: -1, 0, or 1.  A (confusion matrix)[https://en.wikipedia.org/wiki/Confusion_matrix] shows how often the predictor matched each category.  For example, it can be enlightening to see that one heuristic had to guess (predict 0) more often than another.  The confusion matrix also allows us to calculate various stats, such as the accuracy (which heuristica already reports), sensitivity (true positive / all positive), specificity (true negative / all negative) and precision (true positive / predicted positive).  However, there are some quirks to applying confusion matrices to this task, and this vignette explains them.

* Most heuristica functions exploit the fact that row pairs are inherently symmetric (A vs B = - B vs. A), so non-default flags need to be used for confusion matrix output.
* Guesses and ties need to be categorized as 1 or -1 for measures like accuracy.


#' accuracy = (true positive + true negatve) / all
#' sensitivity = true pasitive rate = true positive / all positive
#'   (sensitivity is also called recall)
#' specificity = true negative rate = true negative / all negative
#' precision = positive predictive velue = true positive

# Running reverse row pairs

```{r}
data("city_population")
data_set <- na.omit(city_population)
criterion_col <- 3
cols_to_fit <- 4:ncol(data_set)
```

Fit Take the Best and regression on a subset of the data.  (For this example, some interested rows were selected, but in practice the training rows would be randomly sampled.  Also, in practice we would want to check predictions on the non-training data.)

```{r}
num_training_rows <- 5
train_data <- city_population[c(3:(3+num_training_rows)),]
ttb <- ttbModel(train_data, criterion_col, cols_to_fit)
reg <- regModel(train_data, criterion_col, cols_to_fit)
lreg <- logRegModel(train_data, criterion_col, cols_to_fit)
```

## Analyzing just "forward" row pairs

We normally use `predictPairSummary`, but that efficiently applies the functions to only one set of unique row pairs.  For example, the output below shows CorrectGreater as only having the output of 1 because the data is sorted in descening order.  But for a confusion matrix, we also want to count what the heuristics would have predicted for -1 rows.

```{r}
out_fwd_row_pairs_only <- predictPairSummary(train_data, ttb, reg, lreg)
out_fwd_row_pairs_only
```

Just for kicks-- even though it's incomplete-- let's look at a confusion matrix for TakeTheBest.  Notice only the last row, named "1" has values.  That is because the only category in "ref_data" was "1".  (The function still produces the other rows so that we always get the same 3x3 shape for analysis.)

```{r}
ref_data <- out_fwd_row_pairs_only[,"CorrectGreater"]
predictions <- out_fwd_row_pairs_only[,"ttbModel"]
confusionMatrixFor_Neg1_0_1(ref_data, predictions)
```

The fix is to generate row pairs for "backward" row pairs, too, e.g. Row2 vs. Row 1 in addition to Row 1 vs. Row 2.

## Generating "reverse" row pairs

There are several ways to generate the confusion matrix to include the "reverse" row pairs. 

The most straightforward way to handle this is to run predictPairSummary twice, once with the rows in the usual "forward" order and then again with the rows in "reverse order."  Below we see that CorrectGreater now includes -1's.

```{r}
out_fwd_row_pairs_only <- predictPairSummary(train_data, ttb, reg, lreg)
train_data_rev_rows <- train_data[c(nrow(train_data):1),]
out_rev_row_pairs_only <- predictPairSummary(train_data_rev_rows, ttb, reg, lreg)
out <- rbind(out_fwd_row_pairs_only, out_rev_row_pairs_only)
out
```

With that output, we can generate a complete confusion matrix for Take The Best with non-zero data in both the first "-1" row and the last "1" row.

```{r}
ref_data <- out[,"CorrectGreater"]
predictions <- out[,"ttbModel"]
confusionMatrixFor_Neg1_0_1(ref_data, predictions)
```

You might also notice a symmetry.  For example, there are 7 cases where TTB accurately predicted 1 where the value was 1.  When these rows were reversed, they resulted in 7 cases where TTB accurately predicted -1 where the correct value was -1.  This happens because TTB is a symmetric model: predictPair(Row1, Row2) = - predictPair(Row2, Row1).  All the models included in heuristica have this property.  We can exploit it directly in calculating the confusion matrix.  A hand-coded example is below.

```{r}
ref_data <- out_fwd_row_pairs_only[,"CorrectGreater"]
predictions <- out_fwd_row_pairs_only[,"ttbModel"]
ttb_fwd_confusion_matrix <- confusionMatrixFor_Neg1_0_1(ref_data, predictions)
ttb_confusion_matrix <- ttb_fwd_confusion_matrix
ttb_confusion_matrix["-1", "-1"] <- ttb_fwd_confusion_matrix["1", "1"]
ttb_confusion_matrix["-1", "0"] <- ttb_fwd_confusion_matrix["1", "0"]
ttb_confusion_matrix["-1", "1"] <- ttb_fwd_confusion_matrix["1", "-1"]
ttb_confusion_matrix
```

Yet another way is to use the more flexible rowPairApplyList function, which can take optional arguments.  Setting `also_reverse_row_pairs=TRUE` is key for producing output where CorrectGreater is -1.

```{r}
out <- rowPairApplyList(train_data, list(correctGreater(criterion_col), heuristics(ttb, reg, lreg)), also_reverse_row_pairs=TRUE)
out
```

We can look at Take The Best's confusion matrix and confirm it agrees with those above.

```{r}
ref_data <- out[,"CorrectGreater"]
predictions <- out[,"ttbModel"]
confusionMatrixFor_Neg1_0_1(ref_data, predictions)
```

## Analyzing output of the three models

Let's generate a confusion matrices for each heuristic.
```{r}
confusion_matrix_3x3_ttb <- confusionMatrixFor_Neg1_0_1(out[,"CorrectGreater"], out[,"ttbModel"])
confusion_matrix_3x3_ttb

confusion_matrix_3x3_reg <- confusionMatrixFor_Neg1_0_1(out[,"CorrectGreater"], out[,"regModel"])
confusion_matrix_3x3_reg

confusion_matrix_3x3_lreg <- confusionMatrixFor_Neg1_0_1(out[,"CorrectGreater"], out[,"logRegModel"])
confusion_matrix_3x3_lreg
```

Interestingly, regression and logistic regression have the exact same confusion matrix, even though the output showed they sometimes disagreed.  Below are the cases where they diagreed.
```{r}
out_df <- data.frame(out)
out_df[out_df$regModel != out_df$logRegModel,]
```


======

First, notice that the first and third rows are the same.  This is becuase ttb and regression are symmetric heuristics.  When a symmetric heuristic chooses Berlin when given Berlin vs. Munich-- an output of "1" in predictPair-- it will still choose Berlin when given the reverse, Munich vs. Berlin-- and output of "-1" in predictPair.  It's also confirmation that we set `also_reverse_row_pairs=TRUE`.  If you forget to set this, the first and third rows might not be equal.

Second, notice that Take The Best guesses a lot more.  Focusing on the first row, Take The Best guessed 1,213 times while regression guessed roughly half as much 678 times.

Now view both matrices with guesses-- 0 predictions-- allocated.  There are different ways to allocate guesses, but by default this fucntion assignes 50% of them to correct and 50% to incorrect.
```{r}
confusion_matrix_ttb <- collapseConfusionMatrix3x3To2x2(confusion_matrix_3x3_ttb)
confusion_matrix_ttb
confusion_matrix_reg <- collapseConfusionMatrix3x3To2x2(confusion_matrix_3x3_reg)
confusion_matrix_reg
```


Calculate stats, including accuracy from these confusion matrices.

```{r}
statsFromConfusionMatrix(confusion_matrix_ttb)
statsFromConfusionMatrix(confusion_matrix_reg)
```


# Analyzing a Guttman environment

This is an even more advanced example inspired by the paper, "The Bias Bias" (citation below).

A confusion matrix compares the predicted values with the actual, correct values.  rowPairApply can generate these values, and here we walk through an example, comparing Take the Best and Greedy Take The Best on a Guttman environment, which has every possible binary cue of validity one.  This is one of the cases analyzed in the paper, "The Bias Bias" (citation below).

Here is a function to generate a Guttman environment:
```{r}
# Makes a Guttman binary data set.  E.g.
# > makeGuttman(3)
#      y   x1  x2 x3
# [1,] 4   1   1   1
# [2,] 3   0   1   1
# [3,] 2   0   0   1
# [4,] 1   0   0   0
# Not efficient, but it works.
makeGuttman <- function(num_cues) {
  num_rows <- num_cues + 1
  mat <- cbind(y=c(num_rows:1))
  for (num_1s in 1:num_cues) {
    num_0s <- num_rows - num_1s
    vec <- c(rep(1, num_1s), rep(0, num_0s))
    mat <- cbind(mat, vec)
    colnames(mat)[ncol(mat)] <- paste0("x", num_1s)
  }
  return(mat)
}
```

Now use it to generate a data set with 32 cues (33 rows).
```{r}
all_data <- makeGuttman(4)
```

Fit the Take The Best model and confirm it thinks all cue validities are 1.
```{r}
train_data <- all_data
criterion_col <- 1
ttb <- ttbModel(train_data, criterion_col, c(2:ncol(train_data)))
ttb$cue_validities
```

To generate data for the confusion matrix, we need correct values, which can be generated with correctGreater, and the predictions from the ttb model.  Generate it like this:

```{r}
out <- rowPairApplyList(train_data, list(correctGreater(criterion_col), heuristics(ttb)), also_reverse_row_pairs=FALSE)
```

Now send the data to the confusion matrix function.

```{r}
confusion_matrix <- confusionMatrixFor_Neg1_0_1(out[,"CorrectGreater"], out[,"ttbModel"])
#confusion_matrix
```



Now let's us

Notice the optional parameter `also_reverse_row_pairs=TRUE`.

Why is this wrong?  rowPairApply tries to be efficient, so be default it runs each unique row pair only once.  That is, if it applies a function to row1 vs. row2, it does not also apply it to row2 vs. row1.  That works well when both these conditions hold:

1) Your heuristics are symmetric, e.g. predictPair(, B, heuristic) = -predictPair(row1, row2, heuristic).  (A heuristic that always predicts 1 violatees this.)
2) Your goal is measuring accuracy, # correct predictions / # predictions.

Condition 2 is violated when calculating a confusion matrix because it shows specifically what happens for _all_ outcomes.  So in order to generate data for a confusion matrix, it is important to use the optional `also_reverse_row_pairs=TRUE` parameter.


