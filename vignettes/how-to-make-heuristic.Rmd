---
title: "How to make your own heuristic"
author: "Jean Whitmore"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{How to make your own heuristic}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

So you have your own idea for a heuristic.  Just implement a few functions and you can run
comparisons with heuristica's heuristics.

## Installation

To get the github version of heuristica:

```{r}
# Uncomment and execute install functions you may need.
# install.packages("devtools") 
# Uncomment and execute 
# install.packages("heuristica") 
#    -- OR --
# devtools::install_github("jeanimal/heuristica")
#    -- OR --
devtools::load_all()
library("heuristica")
```

## Minimal model

### Fitting function

First, write a function to fit data.  It must have three required arguments, but then you can have as many optional arguments as you want.  The required arguments are:
* train_data.  This is the data to train on and can be either a matrix or data.frame
* criterion_col.  The index of the criterion column, the "Y" in a regression.
* cols_to_fit.  A vector of indexes of columns to fit, the "X's" in a regression.

This function should output a structure with several elements:
* criterion_col for heuristica to use
* cols_to_fit for heuristica to use
* A class name the same as your function name, "myModel" in this case.
* Any information your predictor function will need, such as fitted parameters
So let's see a bare bones example.

```{r}
myModel <- function(train_data, criterion_col, cols_to_fit) {
  # We will fill in a more interesting version below.
  structure(list(criterion_col=criterion_col, cols_to_fit=cols_to_fit),
            class="myModel")
}
```

The class name will tell heuristica how to find the predicting function.  But if you are curious, you can read more about S3 classes at http://adv-r.had.co.nz/OO-essentials.html.

### Predicting function

To make predictions with heuristica, implement a function called predictProbInternal, then a dot, then your class name.  Inputs:
* object, which will have everything returned by your fitting function
* row1, one row of a matrix or data.frame having just the columns in cols_to_fit
* row2, another row of a matrix or data.frame having just the columns in cols_to_fit
The output should be:
* A value from 0 to 1, the probability that row1's criterion is greater than row2's criterion.

For our bare-bones example, we will randomly guess 0 or 1.  1 means we predict the criterion in row1 will be greater.  0 means we predict the criterion in row2 will be greter.

```{r}
predictProbInternal.myModel <- function(object, row1, row2) {
  return(sample(c(0,1), 1))
}
```

### Using the new model

Let's consider a subset of the high school dropout data included with this package.  This subset has 5 schools.  The first column has the school name.  The drop-out rates are in column 2, and we will fit them using columns 3-5, namely Enrollment, Attendance Rate, and Low Income Students.
```{r}
data("highschool_dropout")
schools <- highschool_dropout[c(1:5), c(1,4,6,7,11)]
schools
```

To analyze our heurist on this data requires these steps:
1. "Fit" myModel to the schools data, which in this case does nothing.
2. Ask the model to predict the probability that Austin has a higher dropout rate than Farrgut (the first two schools in our data).

```{r}
myFit <- myModel(schools, 2, c(3:5))
row1 <- oneRow(schools, 1)
row1
row2 <- oneRow(schools, 2)
row2
predictPairProb(row1, row2, myFit)
```

We can see in the original data.frame that Austin had the higher dropout rate, meaning 1.0 would have been correct output.  We can get heuristica to show this using a more general rowPair function to view the correct probability based on the criterion, which will be 0 or 1.  (An exception is when the criterion is equal for both values, in which case the correct output is defined as 0.5.)
```{r}
myFit <- myModel(schools, 2, c(3:5))
rowPairApply(oneRow(schools, 1), oneRow(schools, 2), probGreater(2), heuristics(myFit))
```

Heuristica makes this sort of assessment easy-- it can check your model against all row pairs and calculate the percent correct.  Here we do myModel for all pairs of schools in our subset data.  Not surprisingly, it earns roughly 50% correct.
```{r}
myFit <- myModel(schools, 2, c(3:5))
pctCorrectOfPredictPair(list(myFit), schools)
```

## Wrapping lasso regression

That was a toy example.  What if you want to wrap a real model, like lasso regression?  That is implemented in another package, glmnet, so if necessary, install it.
```{r}
# install.packages("glmnet")
library(glmnet)
```

### Fitting function

The fitting function is easy.  Wrap the lasso regression and then add just a little bit of extra information, including the subclass name, criterion column, and columns to fit.  Be sure to keep the class of the original output!

```{r}
lassoModel <- function(train_data, criterion_col, cols_to_fit) {
  # glmnet can only handle matrices, not data.frames.
  cvfit <- suppressWarnings(cv.glmnet(y=as.matrix(train_data[,criterion_col]),
                                      x=as.matrix(train_data[,cols_to_fit])))
  # Make lassoModel a subclass.  Be sure to keep the original class, glmnet.
  class(cvfit) <- c("lassoModel", class(cvfit))
  # Functions in this package require criterion_col and cols_to_fit.
  cvfit$criterion_col <- criterion_col
  cvfit$cols_to_fit <- cols_to_fit
  return(cvfit)
}
```

A fit should now include the extra information we add.
```{r}
my_data <- cbind(y=c(4, 3, 2, 1), x1=c(1.2, 1.1, 1.0, 1.0), x2=c(1, 0, 1, 1))
lasso <- lassoModel(my_data, 1, c(2,3))
lasso$criterion_col
# Should output 1
lasso$cols_to_fit
# Should output 2 3
class(lasso)
# should output "lassoModel" "cv.glmnet"
```

And if you correctly kept the original glmnet class, you can still use all the functions it offers.
```{r}
coef(lasso)
predict(lasso, my_data[,lasso$cols_to_fit])
```

### Predicting function

The task is selecting between two rows.  so lasso should predict each row and choose the one with the higher criterion.  Below is an example implementation or predictProbInternal that will work, although it is not very efficient.
```{r}
predictProbInternal.lassoModel <- function(object, row1, row2) {
  p1 <- predict(object, as.matrix(row1))
  p2 <- predict(object, as.matrix(row2))
  if (p1 > p2) {
    return(1)
  } else if (p1 < p2) {
    return(0)
  } else {
    return(0.5)
  }
}
```

### Using the new model

First, prove we can predict one row pair.

```{r}
predictPairProb(oneRow(my_data, 1), oneRow(my_data, 2), lasso)
```

Now predict all row pairs in our data set.  It got 91% correct-- pretty good!
```{r}
pctCorrectOfPredictPair(list(lasso), my_data)
```

Which comparison did it miss?  Check out the helper function below.  Lasso missed only one comparison, namely row 3 vs. row 4, where it guessed.  It got the other 5 row pairs correctly!
```{r}
allRowPairApply(my_data, rowIndexes(), heuristics(lasso), probGreater(lasso$criterion_col))
```

### Improving performance

If you run lasso's predictions for even a moderately large data set, it will take a while.  For example, for the schools data set, it took 20 seconds on a Macbook air.

The solution is to make the prediction step purely a matrix calculation.  (This might require saving and caching extra information in the fitting step so it does not have to be recalculated on every prediction.)  TODO(Daniel): Paste your optimized code here.
