---
title: "Comparing the predictive performance of models in the heuristica package"
author: "Daniel Barkoczi"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This document provides a simple example of how to compare the predictive performance of different models in the heuristica package. It uses cross-validation on the German cities dataset and measures model performance by the percentage of correct predictions.

**Replication**

```{r}
# Use this seed to exactly replicate my tables and graphs below.
set.seed(3)
# Remove it to see a new sampling-- and whether my overall conclusions still
# hold.

```
**Helper functions**

First let's load the heuristica package to get the heuristics we will compare.  It also includes functions to calculate accuracy.  (The package is only on github for now.)

```{r}
# Uncomment and execute if you do not already have devtools.
#install.packages("devtools")
#devtools::install_github("jeanimal/heuristica")
devtools::load_all() # For development work.
library("heuristica")
```

Now we can load the German cities dataset which is included in the heuristica package.  The criterion column may change depending on your data set, so set it correctly!

```{r}
data(city_population)
data_set <- city_population
criterion_col <- 3
cols_to_fit <- 4:ncol(data_set)
```

Let's enter the models we want to test:
```{r}
vec_of_models <-c(ttbModel, unitWeightModel, validityWeightModel, regModel, logRegModel)
```
Next we define the size of the training and test sets.
```{r}
trainVec <- seq(0.2,0.8,0.2)
# Uncomment the row below if in interactive mode.
#trainVec <- seq(0.1,0.9,0.1)
trainS <- round(nrow(data_set)*trainVec)
testS <- nrow(data_set)-trainS
```
Here's a function that does cross-validation taking the vector of models, criterion column, columns to fit, the dataset, and the number of repetitions as input:
```{r}
crossV <- function(vec_of_models, criterion_col, cols_to_fit, x, train_size, reps, max_test_size) {
  results <- vector()
  for ( i in 1:reps) {
  
    train <- sample(1:nrow(x), train_size)
    test <- setdiff(1:nrow(x), train)
    # Keep test set a constant size for comparable statistics.
    # Note that head will use all data if it has less than max_test_size
    if (max_test_size > length(test)) {
      max_test_size = length(test)
    }
    test <- sample(test, max_test_size)

    training_set <- x[train,]
    test_set <- x[test,]
    
    # If a regression is overdetermined (e.g. has too many columns(), it will
    # drop the right-most columns.  To instead make it drop random columns,
    # we shuffle the column order.
    shuffled_cols_to_fit <- sample(cols_to_fit)

    models<-list()
    y <- 0
    for (mod in vec_of_models) { #fit the models to the training_set
      y <- y+1
      models[[y]] <- mod(training_set, criterion_col, shuffled_cols_to_fit)
    }

    #calculate percentage of correct predictions
    pctCorr <- pctCorrectOfPredictPair(models, test_set)
    results <- rbind(results, pctCorr)
  }

  # Predictions of na are equivalent to guessing, which is 50% correct.
  # TODO(jean): Handle this case in pctCorrectOfPredictPair, not here.
  results[is.na(results)] <- 0.5
  return (colMeans(results))
}  
```

** City population **
Then we can just run this function to calculate predictive accuracy for different training and test set sizes:
```{r}
accuracy<-vector()
reps <- 3
# Uncomment the row below if in interactive mode.
# reps <- 30 
max_test_size <- 8 # Max number of rows in test set to keep stats comparable
for (s in 1:length(trainS)){
  train_size <- trainS[s]
  accuracy <- rbind(accuracy, crossV(vec_of_models, criterion_col, cols_to_fit, data_set, train_size, reps, max_test_size))
}
```
Finally, let's plot the results:
```{r fig.width=7, fig.height=5}
library(reshape)
library(ggplot2)

p <- melt(accuracy)
p$X1 <- rep(100*trainVec, ncol(accuracy))
colnames(p)<-c("size","model","value")

ggplot(p, aes(x=size, y=value, colour=model)) + 
     geom_line() +
     geom_point() + xlab("Size of training set") + ylab("Percent correct")+
     scale_x_continuous(limits=c(10,90),breaks=seq(10,90,10),expand=c(0.01,0.01)) +
     scale_y_continuous(limits=c(min(accuracy)-0.01, max(accuracy)+0.01),breaks=seq(0.6,0.8, 0.1),expand=c(0.01,0.01))

```

** High school drop-outs **
Now do the same analysis for the school drop-out data set.  It has  real-valued cues rather than binary cues.  Also, it has a whopping 23 cues for the 63 Chicago public high schools.

Note that this data set has na's, so we use na.omit to clean them because not all heuristics can handle them properly.
```{r}
data(highschool_dropout)
data_set <- na.omit(highschool_dropout)
criterion_col <- 4
cols_to_fit <- 6:ncol(data_set)
trainVec <- seq(0.1,0.9,0.1)
trainS <- round(nrow(data_set)*trainVec)
testS <- nrow(data_set)-trainS
accuracy<-vector()
reps <- 30
max_test_size <- 8 # Max number of rows in test set to keep stats comparable
for (s in 1:length(trainS)){
  train_size <- trainS[s]
  accuracy <- rbind(accuracy, crossV(vec_of_models, criterion_col, cols_to_fit, data_set, train_size, reps, max_test_size))
  p <- melt(accuracy)
}
p$X1 <- rep(100*trainVec, ncol(accuracy))
colnames(p)<-c("size","model","value")
ggplot(p, aes(x=size, y=value, colour=model)) +
     geom_line() +
     geom_point() + xlab("Size of training set") + ylab("Percent correct")
```

Validity weight model and dawes model do really well at all training set sizes.  Take the Best is not far behind and catches up at about 40% of the training set size.  Regresssion and logistic regression are noticeably weaker, scoring barely above chance for small training sets, when there are more rows than the 23 columns.  They do better as the training set gets larger.  Surprisingly logistic regression does not particularly outperform regression, despite seeming like a more natural fit for two-alternative choice.
